{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 05 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tWhy would you want to use the Data API?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3876c07",
   "metadata": {},
   "source": [
    "<code>The TensorFlow Data API (tf.data) is a powerful and flexible API for building efficient and high-performance input pipelines for your machine learning and deep learning models. Efficient Data Loading, Parallel Processing, Memory Efficiency, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tWhat are the benefits of splitting a large dataset into multiple files?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6fa4f",
   "metadata": {},
   "source": [
    "<code>Splitting a shared database can help improve its performance and reduce the chance of database file corruption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tDuring training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd32d0",
   "metadata": {},
   "source": [
    "<code>Sign:- Low GPU or CPU Utilization, Long Training Epochs,Data Loading Time, Fix:- Batch Size Adjustment, TF API, Data Augmentation & format, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tCan you save any binary data to a TFRecord file, or only serialized protocol buffers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e5afea",
   "metadata": {},
   "source": [
    "<code>The TFRecord format is a simple format for storing a sequence of binary records. Protocol buffers are a cross-platform, cross-language library for efficient serialization of structured data. Protocol messages are defined by . proto files, these are often the easiest way to understand a message type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhy would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a873a4d",
   "metadata": {},
   "source": [
    "<code>which is a specific kind of Protocol Buffers (protobuf) message, is a common practice for certain tasks, such as working with data for machine learning models. we not use own protobuf defination because of Simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tWhen using TFRecords, when would you want to activate compression? Why not do it systematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced32e7",
   "metadata": {},
   "source": [
    "<code>When to Activate Compression: Large Data, Network transfer, Limited Disk Space, etc. When Not to Activate Compression Systematically: Data size, memory useage, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tData can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a1fc0",
   "metadata": {},
   "source": [
    "<code>Preprocessing During Data File Writing:\n",
    "Pros:Data Aggregation, cons:Loss of Flexibility\n",
    "Preprocessing Within tf.data Pipeline:\n",
    "Pros: Dynamic Preprocessing, Cons:Slower Training Start\n",
    "Preprocessing in Preprocessing Layers Within the Model:\n",
    "Pros: Integration with Model, Cons:Inflexible\n",
    "Using TF Transform:\n",
    "Pros: Consistency, Cons:Complex Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5097d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
