{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 10 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tWhat does a SavedModel contain? How do you inspect its content?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SavedModel contains a complete TensorFlow program, including trained parameters (i.e, tf. Variable s) and computation. It does not require the original model building code to run, which makes it useful for sharing or deploying with TFLite, TensorFlow. js, TensorFlow Serving, or TensorFlow Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tWhen should you use TF Serving? What are its main features? What are some tools you can use to deploy it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put simply, TF Serving allows you to easily expose a trained model via a model server. It provides a flexible API that can be easily integrated with an existing system. Most model serving tutorials show how to use web apps built with Flask or Django as the model server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tHow do you deploy a model across multiple TF Serving instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " you can deploy a TensorFlow model across multiple TensorFlow Serving instances, providing scalability and efficient handling of inference requests in a distributed environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tWhen should you use the gRPC API rather than the REST API to query a model served by TF Serving?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of gRPC is its speed and weight compared to the REST APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhat are the different ways TFLite reduces a modelâ€™s size to make it run on a mobile or embedded device?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "efactoring the model involves changing its architecture, reducing the number of layers, or using different layer types that TFLite supports. This can reduce the model's size and computational complexity, making it more suitable for running on smaller devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tWhat is quantization-aware training, and why would you need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization-aware training is a method that allows practitioners to apply quantization techniques without sacrificing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tWhat are model parallelism and data parallelism? Why is the latter generally recommended?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While model parallelism can be beneficial for very large models that do not fit into the memory of a single device, data parallelism is generally recommended for its simplicity, scalability, and efficiency, making it a widely adopted strategy in distributed deep learning frameworks and systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tWhen training a model across multiple servers, what distribution strategies can you use? How do you choose which one to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the choice of distribution strategy often involves experimentation and profiling to determine the most effective approach for a given model and training scenario. Additionally, the size of the model, the computational complexity, and the communication patterns should be taken into account to make an informed decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
