{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 06 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tWhat are the advantages of a CNN over a fully connected DNN for image classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec2f21",
   "metadata": {},
   "source": [
    "<code>No require human supervision required.\n",
    "Automatic feature extraction.\n",
    "Highly accurate at image recognition & classification.\n",
    "Weight sharing.\n",
    "Minimizes computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tConsider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.\n",
    "What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a2c3d",
   "metadata": {},
   "source": [
    "<code>Lowest Layer: 100 feature maps x (3x3x3 parameters) = 27,000 parameters.\n",
    "Middle Layer: 200 feature maps x (3x3x100 parameters) = 1,800,000 parameters.\n",
    "Top Layer: 400 feature maps x (3x3x200 parameters) = 7,200,000 parameters.\n",
    "Total Parameters = 27,000 + 1,800,000 + 7,200,000\n",
    "Total Parameters = 9,027,000 parameters\n",
    "RAM for Single Prediction = 9,027,000 parameters x 4 bytes/parameter x 1 instance\n",
    "RAM for Single Prediction = 36,108,000 bytes or approximately 36.11 megabytes (MB).    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tIf your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a3fc9b",
   "metadata": {},
   "source": [
    "<code>Data Augmentation and Compression, reduce batch size, Gradient Checkpointing, Model Simplification, Use Mixed Precision Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tWhy would you want to add a max pooling layer rather than a convolutional layer with the same stride?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90306a7",
   "metadata": {},
   "source": [
    "<code>Taking the max from the kernel sized patch of an input feature map without having to learn weights for that layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhen would you want to add a local response normalization layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4754ae1",
   "metadata": {},
   "source": [
    "<code>This is useful when we are dealing with ReLU neurons. ReLU neurons have unbounded activations, and we need local response normalization (LRN) to normalize them. To do this, we need to identify high frequency features. By applying LRN, the neurons becomes more sensitive than their neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tCan you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0ca13",
   "metadata": {},
   "source": [
    "<code>AlexNet (2012) vs. LeNet-5 (1998): \n",
    "Deeper Architecture: AlexNet is significantly deeper than LeNet-5, with more convolutional and fully connected layers.\n",
    "Rectified Linear Units (ReLU): AlexNet used rectified linear units as activation functions.\n",
    "Data Augmentation: AlexNet employed extensive data augmentation techniques to reduce overfitting.\n",
    "Local Response Normalization (LRN): AlexNet used LRN, a form of normalization, to improve generalization.\n",
    "Dropout: AlexNet introduced dropout as a regularization technique to prevent overfitting.\n",
    "Parallelization: AlexNet was designed to be run on two GPUs, which allowed it to train faster.\n",
    "GoogleNet: Inception Module(capture feature in various level), Global Average Pooling\n",
    "RESNET: Skip Connections, Batch normalization, Residual Learning\n",
    "SENET: Squeeze-and-Excitation Block(Channel wise feature map), Channel Attention    \n",
    "Xception: Depthwise Separable Convolution, Increase Deapth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tWhat is a fully convolutional network? How can you convert a dense layer into a convolutional layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c685e",
   "metadata": {},
   "source": [
    "<code>A Fully Convolutional Network (FCN) is a type of neural network architecture designed for tasks that involve dense pixel-wise predictions, such as image segmentation and object detection. \n",
    "Convert dense layer into CNN Layer\n",
    "    1: Remove Fully Connected (Dense) Layers\n",
    "    2: Replace with 1x1 Convolution\n",
    "    3: Global Average Pooling (Optional)\n",
    "    4: Set Output size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tWhat is the main technical difficulty of semantic segmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d27c0",
   "metadata": {},
   "source": [
    "<code> High Spatial Resolution, Class Imbalance, real time interface, Large-Scale Data Annotation, Robust, Per-Pixel Labeling, Smantic confusion, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e3af2",
   "metadata": {},
   "source": [
    "#### 9.\tBuild your own CNN from scratch and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04c6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a436b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72117407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 9s 5ms/step - loss: 0.2082 - accuracy: 0.9360 - val_loss: 0.0743 - val_accuracy: 0.9772\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.0602 - accuracy: 0.9815 - val_loss: 0.0501 - val_accuracy: 0.9850\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.0399 - accuracy: 0.9873 - val_loss: 0.0408 - val_accuracy: 0.9880\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.0316 - accuracy: 0.9899 - val_loss: 0.0523 - val_accuracy: 0.9837\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.0250 - accuracy: 0.9919 - val_loss: 0.0442 - val_accuracy: 0.9877\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.0201 - accuracy: 0.9934 - val_loss: 0.0387 - val_accuracy: 0.9887\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.0156 - accuracy: 0.9951 - val_loss: 0.0478 - val_accuracy: 0.9882\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 3s 5ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0507 - val_accuracy: 0.9875\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 3s 5ms/step - loss: 0.0100 - accuracy: 0.9964 - val_loss: 0.0500 - val_accuracy: 0.9870\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.0100 - accuracy: 0.9967 - val_loss: 0.0419 - val_accuracy: 0.9902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22e13bd70a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))  # 10 output classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74fc9cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0365 - accuracy: 0.9892\n",
      "Test accuracy: 98.92%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77bd31",
   "metadata": {},
   "source": [
    "#### 10.\tUse transfer learning for large image classification, going through these steps:\n",
    "- a.\tCreate a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).\n",
    "- b.\tSplit it into a training set, a validation set, and a test set.\n",
    "- c.\tBuild the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation.\n",
    "- d.\tFine-tune a pretrained model on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb380c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b6631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c61743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data augmentation (optional)\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1.0/255.0  # Normalize pixel values to [0, 1]\n",
    ")\n",
    "\n",
    "# Prepare data generators\n",
    "batch_size = 32\n",
    "train_generator = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "val_generator = datagen.flow(x_val, y_val, batch_size=batch_size)\n",
    "test_generator = datagen.flow(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef237cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96_no_top.h5\n",
      "9412608/9406464 [==============================] - 0s 0us/step\n",
      "9420800/9406464 [==============================] - 0s 0us/step\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 27s 17ms/step - loss: 1.5690 - accuracy: 0.0985 - val_loss: 1.2974 - val_accuracy: 0.1571\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.4064 - accuracy: 0.1032 - val_loss: 1.2211 - val_accuracy: 0.0685\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 1.3640 - accuracy: 0.1005 - val_loss: 1.1968 - val_accuracy: 0.0785\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 1.3355 - accuracy: 0.1015 - val_loss: 1.2041 - val_accuracy: 0.1326\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3152 - accuracy: 0.1028 - val_loss: 1.1626 - val_accuracy: 0.1057\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.3076 - accuracy: 0.1025 - val_loss: 1.1595 - val_accuracy: 0.1198\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.2868 - accuracy: 0.1038 - val_loss: 1.1469 - val_accuracy: 0.1205\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2829 - accuracy: 0.1018 - val_loss: 1.1506 - val_accuracy: 0.1098\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2740 - accuracy: 0.1042 - val_loss: 1.1740 - val_accuracy: 0.0932\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 18s 14ms/step - loss: 1.2616 - accuracy: 0.1019 - val_loss: 1.1538 - val_accuracy: 0.1052\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 39s 28ms/step - loss: 1.7181 - accuracy: 0.1091 - val_loss: 1.5551 - val_accuracy: 0.0595\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 33s 27ms/step - loss: 1.2181 - accuracy: 0.1032 - val_loss: 1.0731 - val_accuracy: 0.0922\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 1.0543 - accuracy: 0.1042 - val_loss: 0.8427 - val_accuracy: 0.0973\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.9543 - accuracy: 0.1045 - val_loss: 0.8016 - val_accuracy: 0.1019\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8705 - accuracy: 0.1037 - val_loss: 0.7273 - val_accuracy: 0.0888\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 31s 24ms/step - loss: 0.8168 - accuracy: 0.1035 - val_loss: 0.6955 - val_accuracy: 0.0978\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7681 - accuracy: 0.1037 - val_loss: 0.6595 - val_accuracy: 0.0854\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 31s 24ms/step - loss: 0.7246 - accuracy: 0.1038 - val_loss: 0.6560 - val_accuracy: 0.0772\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.6958 - accuracy: 0.1032 - val_loss: 0.5853 - val_accuracy: 0.1052\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.6619 - accuracy: 0.1033 - val_loss: 0.6009 - val_accuracy: 0.1071\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6167 - accuracy: 0.1111\n",
      "Test accuracy: 11.11%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model without the top (classification) layer\n",
    "base_model = MobileNetV2(input_shape=(96, 96, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Add custom classification layers on top of the base model\n",
    "model = models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='softmax'))  # 10 classes in CIFAR-10\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=epochs)\n",
    "\n",
    "# Unfreeze some layers for fine-tuning (optional)\n",
    "base_model.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_epochs = 10\n",
    "history_fine_tune = model.fit(train_generator, validation_data=val_generator, epochs=fine_tune_epochs)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5e248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
